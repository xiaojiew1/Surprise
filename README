################################################################
# drrec
################################################################
ssh xiaojie@10.100.229.246 # cpu xw # song
ssh xiaojie@10.100.228.181 # gpu xw # coat, risk
ssh xiaojie@10.100.228.158 # gpu cz # coat, risk


conda create -n drrec python=3.6
pip install --ignore-installed -r requirements.txt
pip install -e .

################################################################
# learning theory
################################################################
http://www.shivani-agarwal.net/Teaching/E0370/Aug-2011/Lectures/3.pdf

################################################################
# latex
################################################################
We use the following two methods for propensity estimation~\cite{schnabel2016recommendations}.
\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
\item \mytextbf{Naive Bayes}.
Suppose propensities are conditioned on the true ratings only, based on Bayes theorem, we have
\begin{equation*}
\anEstPropensity
=P(\anObservation=1|\aTrueRating)
=\frac{P(\aTrueRating|\anObservation=1)P(\anObservation=1)}{P(\aTrueRating)}
\text{.}
\end{equation*}%
We estimate $P(\aTrueRating|\anObservation=1)$ and $P(\anObservation=1)$ by maximum likelihood on MNAR data.
To estimate $P(\aTrueRating)$, we need to use a small sample of MAR data.
\item \mytextbf{Logistic Regression}.
Given a vector $\observedFeature$ that encodes all observable features about a user-item pair, we estimate propensities based on logistic regression as follows,
\begin{equation*}
\anEstPropensity=\sigma(\logRegrFeatWeight^\transpose\observedFeature+\logRegrUserBias+\logRegrItemBias)
\text{.}
\end{equation*}%
Here, $\sigma(\cdot)$ is the sigmoid function. $\logRegrFeatWeight$, $\logRegrUserBias$, and $\logRegrItemBias$ are trainable parameters.
\end{enumerate}

the SNIPS estimator~\cite{swaminathan2015self} defined as.
\begin{equation*}
\snipsEstimator=
\frac{1}{\verbSumUsersItems\frac{\anObservation}{\anEstPropensity}}
\verbSumUsersItems\frac{\anObservation\aTrueError}{\anEstPropensity}
\text{.}
\end{equation*}%
