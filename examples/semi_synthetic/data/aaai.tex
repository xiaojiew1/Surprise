
\section{Double Robust Risk Estimator} \label{sec:double robust risk estimator}
Let us consider the problem of recommending relevant items from a candidate pool of items $\ITEM\in\{1,...,\itemnum\}$ to a number of users $\USER\in\{1,...,\usernum\}$.
Suppose the true ratings $\trueratings$ for all items by all users  are fully observed, the true loss of a recommendation system is defined as~\cite{koren2009matrix}
\begin{equation}
\label{equ:true loss}
\begin{aligned}
\trueloss{\predratings}
=
\normconst\sumuseritem\trueimputation
=
\normconst\sumuseritem(\truerating-\predrating)^{2}
\text{,}
\end{aligned}
\end{equation}%
where $\predratings$ is the predicted ratings of the recommendation system.
However, the true ratings are partially observed in the real world, a naive estimator of the true loss is to use the average over only the observed true ratings as
\begin{equation}
\begin{aligned}
\naiveloss{\predratings}
=
\frac{1}{\sumuseritem\observation}\sumuseritem\observation\trueimputation
\text{,}
\end{aligned}
\end{equation}%
where an observation $\observation=1$ if and only if the true rating $\truerating$ is observed and $\observation=0$ otherwise.
It has been found that the observed true ratings are usually biased~\cite{marlin2007collaborative}, e.g., high ratings are much more likely to be observed than low ratings~\cite{marlin2009collaborative}.
Because of the biased observations, the naive estimator is not an unbiased estimate of the true loss in most cases~\cite{steck2013evaluation}.
Formally, assume that the observations are independent and each observation follows a Bernoulli distribution $\observation\sim\bernoulli{\truepropensity}$, we have
\begin{equation}
\begin{aligned}
\EXPECTATION{\observations}{\naiveloss{\predratings}}
=
\frac{1}{\sumuseritem\observation}\sumuseritem\truepropensity\trueimputation
\neq
\trueloss{\predratings}
\text{.}
\end{aligned}
\end{equation}%

To obtain an unbiased estimate of the true loss, we can use either an imputation-based estimator~\cite{steck2010training} or a propensity-based estimator~\cite{schnabel2016recommendations}.
An imputation-based estimator is defined as 
\begin{equation}
\begin{aligned}
\imputationloss{\predratings}
=
\normconst\sumuseritem
\big(
\observation\trueimputation
+
(1-\observation)\predimputation
\big)
\text{,}
\end{aligned}
\end{equation}%
where $\predimputation\approx\trueimputation$ is an imputation model that aims to predict the true loss regarding the unobserved true ratings.
Suppose the imputation model is accurate $\predimputation=\trueimputation$, the imputation-based estimator becomes an unbiased estimate of the true loss because
\begin{equation}
\begin{aligned}
&
\EXPECTATION{\observations}{\imputationloss{\predratings}}
\\
&\quad
=
\normconst\sumuseritem
\big(
\EXPECTATION{\observations}{\observation\trueimputation}
+
\EXPECTATION{\observations}{(1-\observation)\predimputation}
\big)
\\
&\quad
=
\normconst\sumuseritem
\big(
\truepropensity\trueimputation
+
(1-\truepropensity)\predimputation
\big)
\\
&\quad
=
\trueloss{\predratings}
\text{.}
\end{aligned}
\end{equation}%
Alternatively, we can use a propensity-based estimator as 
\begin{equation}
\begin{aligned}
\propensityloss{\predratings}
=
\normconst\sumuseritem
\frac{\observation\trueimputation}{\predpropensity}
\text{,}
\end{aligned}
\end{equation}%
where $\predpropensity\approx\truepropensity$ is a propensity model that aims to predict the true probability of observing the true ratings.
Suppose the propensity model is accurate $\predpropensity=\truepropensity$, the propensity-based estimator becomes an unbiased estimate of the true loss because
\begin{equation}
\begin{aligned}
\EXPECTATION{\observations}{\propensityloss{\predratings}}
&
=
\normconst\sumuseritem
\EXPECTATION{\observations}{\frac{\observation\trueimputation}{\predpropensity}}
\\
&
=
\normconst\sumuseritem
\frac{\truepropensity\trueimputation}{\predpropensity}
\\
&
=
\trueloss{\predratings}
\text{.}
\end{aligned}
\end{equation}%

We propose a double robust estimator which is an unbiased estimator of the true loss when either the imputation model or the propensity model is accurate.
The double robust estimator is defined as
\begin{empheq}{align}
\doublerobustloss{\predratings}
&=
\normconst\sumuseritem
\big(
\predimputation
+
\frac{\observation}{\predpropensity}(\trueimputation-\predimputation)
\big)
\label{equ:imputation double robust}
\\
&=
\normconst\sumuseritem
\big(
\frac{\observation\trueimputation}{\predpropensity}
+
\frac{\predpropensity-\observation}{\predpropensity}\predimputation
\big)
\label{equ:propensity double robust}
\text{.}
\end{empheq}%
Suppose the imputation model is accurate, with the double robust estimator in Eq. \ref{equ:imputation double robust}, we have
\begin{equation}
\begin{aligned}
&
\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
\\
&\quad
=
\normconst\sumuseritem
\big(
\EXPECTATION{\observations}{\predimputation}
+
\EXPECTATION{\observations}{\frac{\observation}{\predpropensity}(\trueimputation-\predimputation)}
\big)
\\
&\quad
=
\normconst\sumuseritem
\big(
\predimputation
+
\frac{\truepropensity}{\predpropensity}(\trueimputation-\predimputation)
\big)
\\
&\quad
=
\trueloss{\predratings}
\text{.}
\end{aligned}
\end{equation}%
On the other hand, suppose the propensity model is accurate, with the double robust estimator in Eq. \ref{equ:propensity double robust}, we have
\begin{equation}
\begin{split}
\raisetag{3.95\baselineskip}
&
\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
\\
&\quad
=
\normconst\sumuseritem
\big(
\EXPECTATION{\observations}{\frac{\observation\trueimputation}{\predpropensity}}
+
\EXPECTATION{\observations}{\frac{\predpropensity-\observation}{\predpropensity}\predimputation}
\big)
\\
&\quad
=
\normconst\sumuseritem
\big(
\frac{\truepropensity\trueimputation}{\predpropensity}
+
\frac{\predpropensity-\truepropensity}{\predpropensity}\predimputation
\big)
\\
&\quad
=
\trueloss{\predratings}
\text{.}
\end{split}
\end{equation}%

\subsection{Bias of Double Robust Estimator}
We turn to the case where both the imputation model and the propensity model are inaccurate.
The following characterizes the bias that the inaccurate imputation model and the inaccurate propensity model induce.
\begin{lemma}
\label{lem:bias}
Let $\predimputation$ be an imputation model that does not perfectly predict the true loss $\trueimputation$.
Let $\predpropensity$ be a propensity model that does not perfectly predict the true probability $\truepropensity$.
The bias of the double robust estimator is given by
\begin{equation}
\hspace*{-0.16cm}
\text{bias}(\doublerobustloss{\predratings})
=
\normconst\sumuseritem
\frac{(\predpropensity-\truepropensity)(\trueimputation-\predimputation)}{\predpropensity}
\end{equation}%
\end{lemma}%
\begin{proof}
We define the bias of the double robust estimator as
\begin{equation}
\text{bias}(\doublerobustloss{\predratings})
=
\trueloss{\predratings}
-
\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
\end{equation}%
Expanding the second term yields
\begin{equation}
\label{equ:double robust estimator expectation}
\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
=
\normconst\sumuseritem
\big(
\predimputation
+
\frac{\truepropensity}{\predpropensity}(\trueimputation-\predimputation)
\big)
\end{equation}%
After subtracting Eq. \ref{equ:double robust estimator expectation} from Eq. \ref{equ:true loss}, we have
\begin{equation}
\begin{aligned}
&\text{bias}(\doublerobustloss{\predratings})
\\
&\quad
=
\normconst\sumuseritem
\big(
(\trueimputation-\predimputation)
-
\frac{\truepropensity}{\predpropensity}(\trueimputation-\predimputation)
\big)
\\
&\quad
=
\normconst\sumuseritem
\frac{(\predpropensity-\truepropensity)(\trueimputation-\predimputation)}{\predpropensity}
\text{,}
\end{aligned}
\end{equation}%
which completes the proof.
\end{proof}
The following show that the double robust estimator can have a smaller bias than the propensity-based estimator so long as the imputation model is not grossly inaccurate.
\begin{theorem}
\label{thm:bias}
Suppose the estimated losses are bounded by $0\leq\predimputation\leq 2\trueimputation$, the absolute value of the bias of the double robust estimator $\doublerobustloss{\predratings}$ is smaller than that of the propensity-based estimator $\propensityloss{\predratings}$.
\end{theorem}%
\begin{proof}
\begin{equation*}
\begin{aligned}
&
0\leq\predimputation\leq 2\trueimputation
% \\&\quad\Rightarrow
% |\trueimputation-\predimputation|\leq\trueimputation
\\&\quad\Rightarrow
\sumuseritem
\frac{|\predpropensity-\truepropensity||\trueimputation-\predimputation|}{\predpropensity}
\leq
\sumuseritem
\frac{\trueimputation|\predpropensity-\truepropensity|}{\predpropensity}
\\&\quad\Rightarrow
\ABSOLUTE{
\text{bias}(\doublerobustloss{\predratings})
}
\leq
\ABSOLUTE{
\text{bias}(\propensityloss{\predratings})
}
\text{,}
\end{aligned}
\end{equation*}%
which completes the proof.
\end{proof}

\subsection{Variance of Double Robust Estimator}
The following characterizes the variability of the double robust estimator.
\begin{lemma}
For any predicted rating $\predratings$, with probability $1-\eta$, the double robust estimator with estimated losses $\predimputations$ and estimated propensities $\predpropensities$ does not deviate from its expectation by more than
\begin{equation}
% \hspace*{-0.16cm}
|\doublerobustloss{\predratings}-\EXPECTATION{\observations}{\doublerobustloss{\predratings}}|
\leq
\sqrt{
\varconst\sumuseritem\intervalsize^{2}
}
\text{,}
\end{equation}%
where $\intervalsize=\frac{\trueimputation-\predimputation}{\predpropensity}$.
\begin{proof}
We define a bounded random variable $\rndvariable$ as
\begin{equation}
\rndvariable
=
\frac{\observation\trueimputation}{\predpropensity}
+
\frac{\predpropensity-\observation}{\predpropensity}\predimputation
\text{,}
\end{equation}%
which takes value in an interval $\interval{\predimputation}{\predimputation+\intervalsize}$ of size $\intervalsize$ with probability 1.
Since the observations are assumed to be independent, according to Hoeffding's inequality, for any $\epsilon$,
\begin{equation*}
P
\left(
\left|
% \sumuseritem\rndvariable-\EXPECTATION{\observations}{\sumuseritem\rndvariable}
\sumuseritem\rndvariable
-
\mathbb{E}_{\observations}
\left[
\sumuseritem\rndvariable
\right]
\right|
\geq\epsilon
\right)
\leq
2\exp
\left(\frac{-2\epsilon^{2}\usernum^{2}\itemnum^{2}}{\sumuseritem\intervalsize^{2}}
\right)
\end{equation*}%
We simplify the above inequality by Eq. \ref{equ:propensity double robust} as
\begin{equation*}
P\left(
\ABSOLUTE{
\doublerobustloss{\predratings}-\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
}
\geq\epsilon
\right)
\leq
2\exp(\frac{-2\epsilon^{2}\usernum^{2}\itemnum^{2}}{\sumuseritem\intervalsize^{2}})
\end{equation*}%
Solving for $\epsilon$ completes the proof.
\end{proof}
\end{lemma}
\begin{theorem}
\label{thm:variance}
Suppose the estimated losses are bounded by $0\leq\predimputation\leq 2\trueimputation$, the tail bound for the double robust estimator is tighter than that of the propensity-based estimator.
\end{theorem}%
\begin{proof}
\begin{equation*}
\begin{aligned}
&
0\leq\predimputation\leq 2\trueimputation
\\&\quad\Rightarrow
\sumuseritem\frac{(\trueimputation-\predimputation)^{2}}{\predpropensity^{2}}
\leq
\sumuseritem\frac{\trueimputation^{2}}{\predpropensity^{2}}
\\&\quad\Rightarrow
\sqrt{
\varconst
\sumuseritem\intervalsize^{2}
}
\leq
\sqrt{
\varconst
\sumuseritem\frac{\trueimputation^{2}}{\predpropensity^{2}}
}
\text{,}
\end{aligned}
\end{equation*}%
which completes the proof.
\end{proof}


\section{Double Robust Learning Framework} \label{sec:dobule robust learning framework}
We use the double robust estimator in an empirical risk minimization framework to train recommendation systems.
Specifically, we use the double robust estimator to estimate the empirical risk for any predicted ratings.
\begin{definition}
Given estimated losses $\predimputations$, estimated propensities $\predpropensities$, a hypothesis space $\hypothesis$ of predicted ratings $\predratings$, we aim to select the $\ermratings$ that optimizes
\begin{equation}
\ermratings
=
\argmin_{\predratings\in\hypothesis}
\doublerobustloss{\predratings}
\end{equation}%
\end{definition}
To illustrate the validity of the empirical risk minimization approach based on the double robust estimator, we state the following generalization error bound.
\begin{theorem}
For any finite hypothesis $\hypothesis$ of predicted ratings, with probability $1-\eta$, the true loss of the $\ermratings$ using the double robust estimator with estimated losses $\predimputations$ and estimated propensities $\predpropensities$ is bounded by
\begin{equation}
\begin{split}
\raisetag{1.75\baselineskip}
&
\trueloss{\ermratings}
\leq
\doublerobustloss{\ermratings}
+
\normconst\sumuseritem
\frac{|\predpropensity-\truepropensity||\trueimputation-\predimputation|}{\predpropensity}
\\
&
\hspace{1.75cm}
+
\sqrt{\ermconst\sumuseritem\intervalsize^{2}}
\text{.}
\end{split}
\end{equation}%
\end{theorem}
\begin{proof}
According to Lemma \ref{lem:bias}, we can rewrite the true loss as
\begin{equation*}
\begin{aligned}
&
\trueloss{\ermratings}
\\&\quad
=
\trueloss{\ermratings}
-
\EXPECTATION{\observations}{\doublerobustloss{\ermratings}}
+
\EXPECTATION{\observations}{\doublerobustloss{\ermratings}}
\\&\quad
=
\text{bias}(\doublerobustloss{\ermratings})
+
\EXPECTATION{\observations}{\doublerobustloss{\ermratings}}
\\&\quad
\leq
\normconst\sumuseritem
\frac{|\predpropensity-\truepropensity||\trueimputation-\predimputation|}{\predpropensity}
+
\EXPECTATION{\observations}{\doublerobustloss{\ermratings}}
\end{aligned}
\end{equation*}%
We are left to bound the following by making a uniform convergence argument via Hoeffding and union bound as
\begin{equation*}
\begin{aligned}
&
P
\left(
\left|
\doublerobustloss{\predratings}-\EXPECTATION{\observations}{\doublerobustloss{\predratings}}
\right|
\leq\epsilon
\right)
\geq1-\eta
\\&\quad\Leftarrow
P
\left(
\max_{\candratings}
\left|
\doublerobustloss{\candratings}-\EXPECTATION{\observations}{\doublerobustloss{\candratings}}
\right|
\leq\epsilon
\right)
\geq1-\eta
\\&\quad\Leftrightarrow
P
\left(
\bigvee_{\candratings}
\ABSOLUTE{
\doublerobustloss{\candratings}-\EXPECTATION{\observations}{\doublerobustloss{\candratings}}
}
\geq\epsilon
\right)
<\eta
\\&\quad\Leftarrow
\sum_{i=1}^{|\hypothesis|}
P
\left(
\ABSOLUTE{
\doublerobustloss{\candratings}-\EXPECTATION{\observations}{\doublerobustloss{\candratings}}
}
\geq\epsilon
\right)
<\eta
\\&\quad\Leftarrow
2|\hypothesis|
\exp(
\frac{-2\epsilon^{2}\usernum^{2}\itemnum^{2}}
{\sumuseritem\intervalsize^{2}}
)
<\eta
\end{aligned}
\end{equation*}%
Solving for $\epsilon$ and adding the bias gives the stated results.
\end{proof}
\begin{theorem}
Suppose the estimated losses are bounded by $0\leq\predimputation\leq 2\trueimputation$, the generalization error bound for the double robust estimator is tighter than that of the propensity-based estimator.
\end{theorem}
\begin{proof}
According to Theorem \ref{thm:bias}, we have
\begin{equation*}
|\text{bias}(\doublerobustloss{\predratings})|
\leq
|\text{bias}(\propensityloss{\predratings})|
\text{.}
\end{equation*}
Analogous to Theorem \ref{thm:variance}, we have
\begin{equation*}
\begin{aligned}
\sqrt{
\ermconst
\sumuseritem\intervalsize^{2}
}
\leq
\sqrt{
\ermconst
\sumuseritem\frac{\trueimputation^{2}}{\predpropensity^{2}}
}
\text{.}
\end{aligned}
\end{equation*}
Adding the bias of the double robust estimator or the propensity-based estimator yields the desired results.
\end{proof}


\section{Double Robust Matrix Factorization} \label{sec:double robust matrix factorization}
